{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:10<17:46, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal_state_values_mean: 0.184\n",
      "optimal_state_values_diff: 1.000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from math import floor\n",
    "from random import random\n",
    "from tqdm import trange\n",
    "\n",
    "from game import init, step, dummy_dealer_stick_policy\n",
    "from plot import plot_2d_value_map, plot_line\n",
    "from value_map import ValueMap\n",
    "\n",
    "EPISODES = int(1e6)\n",
    "BATCH = 100\n",
    "\n",
    "ACTIONS = [\"stick\", \"hit\"]\n",
    "\n",
    "\"\"\"State Value V: state -> expected return\n",
    "\n",
    "state values are learnt from all the trajectory samples\n",
    "from the given state\n",
    "followed by all possible actions given by the policy\n",
    "\"\"\"\n",
    "state_values = ValueMap()\n",
    "\n",
    "\"\"\"Action Value Q: state, action -> expected return\n",
    "\n",
    "action values are learnt from all the trajectory samples\n",
    "from the action on the given state\n",
    "\n",
    "the mean expectation (average mean) also tells us\n",
    "the overall win ratio of the policy at all states\n",
    "when the number of the training episodes is large enough\n",
    "to weight in more later policies\n",
    "\"\"\"\n",
    "action_values = ValueMap()\n",
    "\n",
    "\"\"\"Optimal State Values V*: state -> best action value\n",
    "\"\"\"\n",
    "optimal_state_values = ValueMap()\n",
    "\"\"\"Optimal Policy Pi*: state -> best value action index\n",
    "\"\"\"\n",
    "optimal_policy_values = ValueMap()\n",
    "\n",
    "\n",
    "def get_best_action(state_key):\n",
    "    possible_action_values = [\n",
    "        action_values.get((*state_key, action_index))\n",
    "        for action_index in range(len(ACTIONS))\n",
    "    ]\n",
    "    best_action_value = max(possible_action_values)\n",
    "    best_action_index = possible_action_values.index(best_action_value)\n",
    "    return best_action_index, best_action_value\n",
    "\n",
    "\n",
    "def player_policy(state):\n",
    "    \"\"\"Policy Function: state -> action_index\n",
    "\n",
    "    epsilon-greedy policy is used here\n",
    "    to allow a chance (epsilon) to explore random actions\n",
    "    so that exploration and exploitation is balanced\n",
    "    during the playout learning\n",
    "\n",
    "    epsilon is gradually diminishing to 0\n",
    "    so that when training samples are sufficiently\n",
    "    the policy converges to an optimal policy\n",
    "    choosing the action with max action value\n",
    "\n",
    "    the optimal policy:\n",
    "    pi'(s) = argmax_{a in A} Q(s,a)\n",
    "\n",
    "    throughout the training samples\n",
    "    action value function Q is converging to the true mean\n",
    "    \"\"\"\n",
    "    state_key = (state[\"dealer\"], state[\"player\"])\n",
    "\n",
    "    best_action_index, _ = get_best_action(state_key)\n",
    "\n",
    "    # exploration gradually decreases with more samples\n",
    "    # we use a constant factor N here\n",
    "    # as believed that when sample size of a state\n",
    "    # is significant its value is close to the true value\n",
    "    N = 100\n",
    "    state_count = state_values.count(state_key)\n",
    "    exploration_rate = N / (N + state_count)\n",
    "\n",
    "    if random() < exploration_rate:\n",
    "        return floor(random() * len(ACTIONS))\n",
    "    else:\n",
    "        return best_action_index\n",
    "\n",
    "\n",
    "def learn_episode(sequence, reward):\n",
    "    for [state, action_index] in sequence:\n",
    "\n",
    "        state_key = (state[\"dealer\"], state[\"player\"])\n",
    "        state_values.learn(state_key, reward)\n",
    "\n",
    "        action_key = (*state_key, action_index)\n",
    "        action_values.learn(action_key, reward)\n",
    "\n",
    "\n",
    "def playout():\n",
    "    sequence = []\n",
    "\n",
    "    state = init()\n",
    "\n",
    "    while state[\"reward\"] is None:\n",
    "        player_action_index = player_policy(state)\n",
    "        sequence.append([state, player_action_index])\n",
    "\n",
    "        player_stick = player_action_index == ACTIONS.index(\"stick\")\n",
    "        if player_stick:\n",
    "            break\n",
    "\n",
    "        state = step(state, player_stick)\n",
    "\n",
    "    while state[\"reward\"] is None:\n",
    "        player_stick = True\n",
    "        dealer_stick = dummy_dealer_stick_policy(state)\n",
    "        state = step(state, player_stick, dealer_stick)\n",
    "\n",
    "    reward = state[\"reward\"]\n",
    "\n",
    "    return sequence, reward\n",
    "\n",
    "\n",
    "def set_optimal():\n",
    "\n",
    "    ALL_STATE_KEYS = [\n",
    "        (dealer, player) for player in range(1, 22) for dealer in range(1, 11)\n",
    "    ]\n",
    "\n",
    "    for state_key in ALL_STATE_KEYS:\n",
    "\n",
    "        best_action_index, best_action_value = get_best_action(state_key)\n",
    "\n",
    "        optimal_state_values.set(state_key, best_action_value)\n",
    "        optimal_policy_values.set(state_key, best_action_index)\n",
    "\n",
    "\n",
    "def record_metrics(metrics_history, metrics_instance):\n",
    "    for value_map_name in metrics_history.keys():\n",
    "        value_map = metrics_instance[value_map_name]\n",
    "        for method in metrics_history[value_map_name].keys():\n",
    "            metrics = getattr(value_map, method)()\n",
    "            metrics_history[value_map_name][method].append(metrics)\n",
    "            print(f\"{value_map_name}_{method}: {metrics:.3f}\")\n",
    "\n",
    "\n",
    "def check_convergence(metrics_history, value_map_name, metrics_name):\n",
    "    last_3 = metrics_history[value_map_name][metrics_name][-4:-1]\n",
    "\n",
    "    if len(last_3) < 3:\n",
    "        return False\n",
    "\n",
    "    last_3_mean = sum(last_3) / len(last_3)\n",
    "\n",
    "    if last_3_mean < 0.005:\n",
    "        print(f\"{value_map_name}_{metrics_name} have converged\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    metrics_history = {\n",
    "        \"optimal_state_values\": {\n",
    "            \"mean\": [],\n",
    "            \"diff\": [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    metrics_instance = {\n",
    "        \"optimal_state_values\": optimal_state_values,\n",
    "    }\n",
    "\n",
    "    episodes_count = 0\n",
    "\n",
    "    for _ in trange(BATCH, leave=True):\n",
    "\n",
    "        for _ in range(EPISODES):\n",
    "            sequence, reward = playout()\n",
    "            learn_episode(sequence, reward)\n",
    "\n",
    "        episodes_count += EPISODES\n",
    "        set_optimal()\n",
    "\n",
    "        record_metrics(metrics_history, metrics_instance)\n",
    "        if check_convergence(metrics_history, \"optimal_state_values\", \"diff\"):\n",
    "            break\n",
    "\n",
    "        optimal_state_values.backup()\n",
    "\n",
    "    plot_2d_value_map(optimal_state_values, \"optimal_state_values\", episodes_count)\n",
    "    plot_2d_value_map(optimal_policy_values, \"optimal_policy_values\", episodes_count)\n",
    "    plot_line(metrics_history[\"optimal_state_values_mean\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    train()\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-coursework-Pfwr_Was-py3.6",
   "language": "python",
   "name": "rl-coursework-pfwr_was-py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
