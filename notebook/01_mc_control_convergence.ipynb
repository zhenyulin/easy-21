{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:17<01:15,  1.85s/it]"
     ]
    }
   ],
   "source": [
    "# TASK:\n",
    "# - run monte_carlo_control for 70*1e5 EPISODES (beyond convergence)\n",
    "# - check the convergence order of different value_store\n",
    "# - save true_action_value_store and optimal_state_value_store\n",
    "#\n",
    "# PROCESS:\n",
    "# - monte_carlo_control (e_greedy_policy control & monte_carlo_learning)\n",
    "# - iterate the policy until action_value_store converges and over\n",
    "# - we have the true action values and the optimal state values\n",
    "#\n",
    "# RESULTS:\n",
    "# - target_policy_action_store converges at 26-30, 60+ BATCH*EPISODES(1e5)\n",
    "# - target_state_value_store converges at 56+ BATCH*EPISODES(1e5)\n",
    "# - action_value_store converges at 63, 66+ BATCH*EPISODES(1e5)\n",
    "#\n",
    "# INTERPRETATION:\n",
    "# under convergence condition - mean of last 3 diff < 0.1%\n",
    "# - target_policy_action_store is likely to stuck\n",
    "#   when 3 consecutive samples don't create large enough diff\n",
    "#   to flip the optimal actions\n",
    "# - action_value_store can stuck but not likely for some short term sampling\n",
    "#   that don't generate large enough diff for probably suboptimal actions\n",
    "#   or due to short sequences\n",
    "# - target_state_value_store convergence is stable as optimal actions are\n",
    "#   sampled with priority\n",
    "# - as expected, e_greedy_policy control helps to sample towards optimal\n",
    "#   action/state_values, prioritized sampling is more efficient to find\n",
    "#   optimal (deterministic) policy than full scope sampling\n",
    "#\n",
    "# RUN:\n",
    "# %%\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from tqdm import trange\n",
    "from pprint import pprint\n",
    "\n",
    "from src.agent.model_free_agent import ModelFreeAgent\n",
    "\n",
    "from src.easy_21.game import playout, PLAYER_INFO\n",
    "\n",
    "#\n",
    "# hyperparameters and agent config\n",
    "#\n",
    "BATCH = 50\n",
    "EPISODES = int(1e5)\n",
    "\n",
    "EXPLORATION_RATE = 0.5\n",
    "\n",
    "PLAYER = ModelFreeAgent(\"player\", PLAYER_INFO)\n",
    "\n",
    "#\n",
    "# task process - record the convergence for different value stores\n",
    "#\n",
    "\n",
    "convergence = {\n",
    "    \"target_policy_action_store\": [],\n",
    "    \"target_state_value_store\": [],\n",
    "    \"action_value_store\": [],\n",
    "}\n",
    "\n",
    "for n in trange(BATCH, leave=True):\n",
    "    for _ in range(EPISODES):\n",
    "        playout(\n",
    "            player_policy=lambda state_key: PLAYER.e_greedy_policy(\n",
    "                state_key,\n",
    "                exploration_rate=EXPLORATION_RATE,\n",
    "            ),\n",
    "            player_offline_learning=PLAYER.monte_carlo_learning_offline,\n",
    "        )\n",
    "\n",
    "    PLAYER.set_target_value_stores()\n",
    "\n",
    "    if PLAYER.target_policy_action_store.metrics.record_converged(\"diff\"):\n",
    "        convergence[\"target_policy_action_store\"].append(n)\n",
    "\n",
    "    if PLAYER.target_state_value_store.metrics.record_converged(\"diff\"):\n",
    "        convergence[\"target_state_value_store\"].append(n)\n",
    "\n",
    "    if PLAYER.action_value_store.metrics.record_converged(\"diff\"):\n",
    "        convergence[\"action_value_store\"].append(n)\n",
    "\n",
    "pprint(convergence)\n",
    "PLAYER.target_policy_action_store.metrics.plot_history(\"diff\")\n",
    "PLAYER.target_state_value_store.metrics.plot_history(\"diff\")\n",
    "PLAYER.action_value_store.metrics.plot_history(\"diff\")\n",
    "\n",
    "#\n",
    "# extra:\n",
    "# - save the optimal_state_values, true_action_values\n",
    "# - visualise the optimal state values and policy action\n",
    "#\n",
    "\n",
    "PLAYER.save_target_state_values_as_optimal()\n",
    "PLAYER.action_value_store.save(\"../output/player_true_action_values.json\")\n",
    "PLAYER.plot_2d_target_value_stores()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "rl-coursework-Pfwr_Was-py3.9",
   "language": "python",
   "name": "rl-coursework-pfwr_was-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
