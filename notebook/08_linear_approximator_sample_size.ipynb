{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4091.55it/s]\n",
      " 46%|████▌     | 23/50 [00:39<00:46,  1.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d8123240edbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mPLAYER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_td_lambda_learning_offline_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mPLAYER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_state_value_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/learning/rl-coursework/src/module/model_free_agent.py\u001b[0m in \u001b[0;36mforward_td_lambda_learning_offline_batch\u001b[0;34m(self, episodes, discount, lambda_value, off_policy, mini_batch_size, proxy)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \u001b[0moff_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moff_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                     \u001b[0mdefer_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                     \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                 )\n\u001b[1;32m    318\u001b[0m                 \u001b[0mmini_batch_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/learning/rl-coursework/src/module/model_free_agent.py\u001b[0m in \u001b[0;36mforward_td_lambda_learning_offline\u001b[0;34m(self, episode, discount, lambda_value, off_policy, defer_update, proxy)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproxy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlambda_value\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return self.temporal_difference_learning_offline(\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefer_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m             )\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/learning/rl-coursework/src/module/model_free_agent.py\u001b[0m in \u001b[0;36mtemporal_difference_learning_offline\u001b[0;34m(self, episode, discount, off_policy, defer_update)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0moff_policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     else self.action_value_store.get(\n\u001b[0;32m--> 173\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstate_key_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_index_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                     )\n\u001b[1;32m    175\u001b[0m                 )\n",
      "\u001b[0;32m~/Desktop/learning/rl-coursework/src/lib/value_approximator.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, input, output_features)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mregarded\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights_if_not_yet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/learning/rl-coursework/src/lib/value_approximator.py\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_weights_if_not_yet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TASK:\n",
    "# - check table_lookup/full_binary_feature x sample_size ~ converge\n",
    "#\n",
    "# PROCESS;\n",
    "# - sample different EPISODES experiences with with table_lookup/full_binary_feature\n",
    "# - replay experience to fit with value approximator\n",
    "# - check accuracy history\n",
    "#\n",
    "# RESULTS:\n",
    "# - for 1e4 EPISODES, 2e4 EPISODES binary_feature doesn't show much difference\n",
    "#   in accuracy\n",
    "# - for 1e4 EPISODES, 2e4 EPISODES table_lookup show obviously better accuracy\n",
    "#   with more samples\n",
    "#\n",
    "# INTERPRETATION:\n",
    "# - linear combination of features has certain limit of accuracy as well as\n",
    "#   some limitation in absorb new information from more samples\n",
    "#\n",
    "# RUN:\n",
    "# %%\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from tqdm import trange\n",
    "from random import shuffle\n",
    "\n",
    "from src.module.model_free_agent import ModelFreeAgent\n",
    "\n",
    "from src.easy_21.game import playout, ACTIONS, STATE_LABELS, PLAYER_STATES\n",
    "from src.easy_21.feature_function import table_lookup, full_binary_feature\n",
    "\n",
    "#\n",
    "# hyperparameters and agent config\n",
    "#\n",
    "\n",
    "EPOCH = 50\n",
    "\n",
    "PLAYER = ModelFreeAgent(\n",
    "    \"player\",\n",
    "    ACTIONS,\n",
    "    STATE_LABELS,\n",
    "    PLAYER_STATES,\n",
    "    state_compressor=table_lookup,\n",
    ")\n",
    "PLAYER.load_optimal_state_values()\n",
    "\n",
    "PLAYER.target_state_value_store.metrics_methods[\n",
    "    \"accuracy\"\n",
    "] = PLAYER.target_state_value_store_accuracy_to_optimal\n",
    "\n",
    "#\n",
    "# process\n",
    "#\n",
    "\n",
    "for EPISODES in [int(1e4), int(2e4)]:\n",
    "    for feature_function in [table_lookup, full_binary_feature]:\n",
    "\n",
    "        PLAYER.action_value_store.reset()\n",
    "        PLAYER.action_value_store.feature_function = feature_function\n",
    "\n",
    "        experiences = [\n",
    "            playout(player_policy=PLAYER.e_greedy_policy)[0] for _ in trange(EPISODES)\n",
    "        ]\n",
    "\n",
    "        for _ in trange(EPOCH):\n",
    "            shuffle(experiences)\n",
    "            PLAYER.forward_td_lambda_learning_offline_batch(experiences)\n",
    "\n",
    "            PLAYER.target_state_value_store.record(\"accuracy\", log=False)\n",
    "\n",
    "        PLAYER.target_state_value_store.stack_metrics_history(\"accuracy\")\n",
    "\n",
    "labels = [\n",
    "    \"1e4 episodes-table_lookup\",\n",
    "    \"1e4 episodes-binary_feature\",\n",
    "    \"2e4 episodes-table_lookup\",\n",
    "    \"2e4 episodes-binary_feature\",\n",
    "]\n",
    "PLAYER.target_state_value_store.plot_metrics_history_stack(\n",
    "    \"accuracy\",\n",
    "    labels=labels,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
